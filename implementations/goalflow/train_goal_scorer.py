"""
GoalPointScorer è®­ç»ƒè„šæœ¬

è®­ç»ƒæµç¨‹ï¼š
1. åŠ è½½æ•°æ®é›†å’Œè¯æ±‡è¡¨
2. åˆå§‹åŒ–æ¨¡å‹ã€ä¼˜åŒ–å™¨
3. è®­ç»ƒå¾ªç¯ï¼š
   - å‰å‘ä¼ æ’­ï¼šé¢„æµ‹ distance score å’Œ DAC score
   - è®¡ç®—æŸå¤±ï¼šCrossEntropy + BCE
   - åå‘ä¼ æ’­å’Œä¼˜åŒ–
4. éªŒè¯ï¼šè®¡ç®— Top-1/Top-5 å‡†ç¡®ç‡
5. ä¿å­˜æœ€ä½³æ¨¡å‹

TODO: ä½ éœ€è¦å®ç°ä»¥ä¸‹å‡½æ•°
- train_one_epoch()
- validate()
- compute_accuracy()
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
import os
import sys
from tqdm import tqdm

# æ·»åŠ è·¯å¾„
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from models.goal_point_scorer import GoalPointScorer
from data.toy_goalflow_dataset import ToyGoalFlowDataset
from config.scorer_config import ScorerConfig


def set_seed(seed):
    """è®¾ç½®éšæœºç§å­"""
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)


def compute_target_labels(vocabulary: torch.Tensor, 
                          gt_goals: torch.Tensor
                          ) -> torch.Tensor:
    """
    è®¡ç®—è®­ç»ƒæ ‡ç­¾ï¼šæ‰¾åˆ°æœ€æ¥è¿‘ gt_goal çš„ vocabulary ç´¢å¼•
    
    Args:
        vocabulary: (N, 2) è¯æ±‡è¡¨
        gt_goals: (B, 2) çœŸå®ç›®æ ‡ç‚¹
    
    Returns:
        target_idx: (B,) æœ€è¿‘çš„ vocabulary ç´¢å¼•
    
    TODO: å®ç°è¿™ä¸ªå‡½æ•°
    æç¤ºï¼š
    1. è®¡ç®— gt_goals åˆ°æ¯ä¸ª vocabulary ç‚¹çš„è·ç¦»
    2. æ‰¾åˆ°è·ç¦»æœ€å°çš„ç´¢å¼•
    """
    diff = vocabulary.unsqueeze(0) - gt_goals.unsqueeze(1)  # (B, N, 2)
    dis = torch.norm(diff, dim=-1)    # (B, N)
    _, target_idx = torch.min(dis, dim=-1)     # (B,)

    return target_idx


def compute_accuracy(pred_scores: torch.Tensor,
                     target_idx: torch.Tensor, 
                     k: int=1) -> float:
    """
    è®¡ç®— Top-K å‡†ç¡®ç‡
    
    Args:
        pred_scores: (B, N) é¢„æµ‹åˆ†æ•°
        target_idx: (B,) ç›®æ ‡ç´¢å¼•
        k: Top-K
    
    Returns:
        accuracy: float
    """
    # è·å– Top-K é¢„æµ‹ç´¢å¼•
    _, topk_indices = pred_scores.topk(k, dim=-1)  # (B, k)
    
    # æ£€æŸ¥ target_idx æ˜¯å¦åœ¨ Top-K ä¸­
    target_idx_expanded = target_idx.unsqueeze(-1)  # (B, 1)
    correct = (topk_indices == target_idx_expanded).any(dim=-1)  # (B,)
    
    # è®¡ç®—å‡†ç¡®ç‡
    accuracy = correct.float().mean().item()
    
    return accuracy


def train_one_epoch(model: nn.Module, 
                    train_loader: DataLoader, 
                    vocabulary: torch.Tensor, 
                    optimizer: torch.optim.Optimizer, 
                    device, 
                    config):
    """
    è®­ç»ƒä¸€ä¸ª epoch
    
    Args:
        model: GoalPointScorer æ¨¡å‹
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        vocabulary: (N, 2) è¯æ±‡è¡¨
        optimizer: ä¼˜åŒ–å™¨
        device: è®¾å¤‡
        config: é…ç½®
    
    Returns:
        avg_loss: å¹³å‡æŸå¤±
        avg_acc: å¹³å‡å‡†ç¡®ç‡
    
    TODO: å®ç°è®­ç»ƒå¾ªç¯
    æç¤ºï¼š
    1. éå† train_loader
    2. å‰å‘ä¼ æ’­ï¼šmodel.forward()
    3. è®¡ç®—æŸå¤±ï¼šmodel.compute_loss()
    4. åå‘ä¼ æ’­ï¼šloss.backward()
    5. ä¼˜åŒ–å™¨æ›´æ–°ï¼šoptimizer.step()
    6. è®¡ç®—å‡†ç¡®ç‡
    """
    model.train()
    total_loss = 0.0
    total_acc = 0.0
    num_batches = 0

    pbar = tqdm(train_loader, desc="[Train]")  
    for batch in pbar:
        # 1. æ•°æ®ç§»åˆ°è®¾å¤‡
        bev_feature = batch['bev_feature'].to(device)
        gt_goal = batch['goal'].to(device)
        drivable_area = batch['drivable_area'].to(device)

        # 2. è®¡ç®—ç›®æ ‡æ ‡ç­¾
        target_idx = compute_target_labels(vocabulary, gt_goal)

        # 3. å‰å‘ä¼ æ’­
        # æ³¨æ„ï¼švocabulary éœ€è¦æ‰©å±•ä¸º (B, N, 2)
        B = bev_feature.shape[0]
        vocab_expanded = vocabulary.unsqueeze(0).expand(B, -1, -1)  # (B, N, 2)
        pred_dis, pred_dac = model(vocab_expanded, bev_feature)

        # 4. è®¡ç®—çœŸå®æ ‡ç­¾
        true_dis = model.compute_distance_score(vocabulary, gt_goal)  # (B, N)
        true_dac = model.compute_dac_score(vocabulary, drivable_area)  # (B, N)

        # 5. è®¡ç®—æŸå¤±
        loss, loss_dict = model.compute_loss(pred_dis, pred_dac, true_dis, true_dac)
        
        # 6. åå‘ä¼ æ’­å’Œä¼˜åŒ–
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # 7. ç»Ÿè®¡
        total_loss += loss_dict['loss']
        acc = compute_accuracy(pred_dis, target_idx, k=1)
        total_acc += acc
        num_batches += 1
        
        # æ›´æ–°è¿›åº¦æ¡
        pbar.set_postfix({'loss': loss.item(), 'acc': acc})

    return total_loss / num_batches, total_acc / num_batches


def validate(model, 
             val_loader, 
             vocabulary, 
             device, 
             config):
    """
    éªŒè¯æ¨¡å‹
    
    Args:
        model: GoalPointScorer æ¨¡å‹
        val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
        vocabulary: (N, 2) è¯æ±‡è¡¨
        device: è®¾å¤‡
        config: é…ç½®
    
    Returns:
        avg_loss: å¹³å‡æŸå¤±
        top1_acc: Top-1 å‡†ç¡®ç‡
        top5_acc: Top-5 å‡†ç¡®ç‡
    
    TODO: å®ç°éªŒè¯å¾ªç¯
    æç¤ºï¼š
    1. ä½¿ç”¨ torch.no_grad()
    2. è®¡ç®— Top-1 å’Œ Top-5 å‡†ç¡®ç‡
    """
    model.eval()
    total_loss = 0.0
    total_top1_acc = 0.0
    total_top5_acc = 0.0
    num_batches = 0
    
    with torch.no_grad():
        pbar = tqdm(val_loader, desc="[Validate]")  
        for batch in pbar:
            # 1. æ•°æ®ç§»åˆ°è®¾å¤‡
            bev_feature = batch['bev_feature'].to(device)
            gt_goal = batch['goal'].to(device)
            drivable_area = batch['drivable_area'].to(device)

            # 2. è®¡ç®—ç›®æ ‡æ ‡ç­¾
            target_idx = compute_target_labels(vocabulary, gt_goal)

            # 3. å‰å‘ä¼ æ’­
            # æ³¨æ„ï¼švocabulary éœ€è¦æ‰©å±•ä¸º (B, N, 2)
            B = bev_feature.shape[0]
            vocab_expanded = vocabulary.unsqueeze(0).expand(B, -1, -1)  # (B, N, 2)
            pred_dis, pred_dac = model(vocab_expanded, bev_feature)

            # 4. è®¡ç®—çœŸå®æ ‡ç­¾
            true_dis = model.compute_distance_score(vocabulary, gt_goal)  # (B, N)
            true_dac = model.compute_dac_score(vocabulary, drivable_area)  # (B, N)

            # 5. è®¡ç®—æŸå¤±
            loss, loss_dict = model.compute_loss(pred_dis, pred_dac, true_dis, true_dac)

            # 6. ç»Ÿè®¡
            total_loss += loss_dict['loss']
            total_top1_acc += compute_accuracy(pred_dis, target_idx, k=1)
            total_top5_acc += compute_accuracy(pred_dis, target_idx, k=5)
            num_batches += 1
            
            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({'loss': loss.item()})
    
    return (total_loss / num_batches, 
            total_top1_acc / num_batches, 
            total_top5_acc / num_batches)


def main():
    # åŠ è½½é…ç½®
    config = ScorerConfig()
    
    # è®¾ç½®éšæœºç§å­
    set_seed(config.seed)
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device(config.device if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(config.checkpoint_dir, exist_ok=True)
    os.makedirs(config.log_dir, exist_ok=True)
    
    # ==================== åŠ è½½æ•°æ® ====================
    print("Loading dataset...")
    train_dataset = ToyGoalFlowDataset(config.data_path, split='train')
    val_dataset = ToyGoalFlowDataset(config.data_path, split='val')
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.batch_size,
        shuffle=True,
        num_workers=config.num_workers
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.batch_size,
        shuffle=False,
        num_workers=config.num_workers
    )
    
    # è·å–è¯æ±‡è¡¨
    vocabulary = train_dataset.get_vocabulary().to(device)  # (N, 2)
    print(f"Vocabulary size: {vocabulary.shape[0]}")
    
    # ==================== åˆå§‹åŒ–æ¨¡å‹ ====================
    print("Initializing model...")
    model = GoalPointScorer(
        vocabulary_size=config.vocab_size,
        feature_dim=config.hidden_dim,  # ä½¿ç”¨ hidden_dim ä½œä¸º feature_dim
        hidden_dim=config.hidden_dim,
        num_heads=config.num_heads,
        num_layers=config.num_layers,
        scene_in_channels=config.scene_channels,
        kernel_size=3,
        stride=1,
        dropout=config.dropout
    ).to(device)
    
    print(f"Model parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M")
    
    # ==================== ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨ ====================
    optimizer = optim.Adam(
        model.parameters(),
        lr=config.learning_rate,
        weight_decay=config.weight_decay
    )
    
    scheduler = None
    if config.use_scheduler:
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer,
            mode='min',
            factor=config.scheduler_factor,
            patience=config.scheduler_patience
        )
    
    # ==================== è®­ç»ƒå¾ªç¯ ====================
    print("\nStarting training...")
    best_val_loss = float('inf')
    
    for epoch in range(1, config.num_epochs + 1):
        print(f"\n{'='*50}")
        print(f"Epoch {epoch}/{config.num_epochs}")
        print(f"{'='*50}")
        
        # è®­ç»ƒ
        train_loss, train_acc = train_one_epoch(
            model, train_loader, vocabulary, optimizer, device, config
        )
        print(f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}")
        
        # éªŒè¯
        if epoch % config.eval_interval == 0:
            val_loss, top1_acc, top5_acc = validate(
                model, val_loader, vocabulary, device, config
            )
            print(f"Val Loss: {val_loss:.4f}, Top-1 Acc: {top1_acc:.4f}, Top-5 Acc: {top5_acc:.4f}")
            
            # å­¦ä¹ ç‡è°ƒåº¦
            if scheduler is not None:
                scheduler.step(val_loss)
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'val_loss': val_loss,
                    'top1_acc': top1_acc,
                    'top5_acc': top5_acc,
                }, os.path.join(config.checkpoint_dir, 'best.pth'))
                print(f"âœ… Saved best model (val_loss: {val_loss:.4f})")
        
        # å®šæœŸä¿å­˜
        if epoch % config.save_interval == 0:
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
            }, os.path.join(config.checkpoint_dir, f'epoch_{epoch}.pth'))
    
    print("\nğŸ‰ Training completed!")


if __name__ == "__main__":
    main()
