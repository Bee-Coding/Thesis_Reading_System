
=== Page 3 ===
Scorer Decoder
MLP
Perception Module
Goal Point Construction Module
Trajectory Planning Module
Image ğ¼
LiDAR ğ¿
BEV feature
Ego Status
Goal Point 
Vocabulary
MLP
Fusion & Select
Trajectory Decoder
Trajectory Scorer
Dis. Score
DAC Score
Goal Point
Figure 2. Overview of the GoalFlow architecture. GoalFlow consists of three modules. The Perception Module is responsible for
integrating scene information into a BEV feature Fbev, the Goal Point Construction Module selects the optimal goal point from Goal Point
Vocabulary V as guidance information, and the Trajectory Planning Module generates the trajectories by denoising from the Gaussian
distribution to the target distribution. Finally, the Trajectory Scorer selects the optimal trajectory from the candidates.
ing noise have become mainstream.
DDPM[14] applies
noise to images during training, converting states over time
steps, and subsequently denoises them during testing to re-
construct the image. More recent methods[26] have fur-
ther optimized sampling efficiency. Additionally, CFG[13]
has enhanced the robustness of generated outputs. Flow
Matching[23] establishes a vector field for transitioning
from one distribution to another. Rectified flow[24], a spe-
cific form of flow matching, enables a direct, linear tran-
sition path between distributions. Compared to diffusion
models, rectified flow often requires only a single inference
step to achieve good results.
2.3. MultiModal Trajectories Generation
In planning tasks, such as manipulation and autonomous
driving, a given scenario often offers multiple action op-
tions, requiring effective multimodal modeling.
Recent
works[2, 31] in manipulation have explored this by applying
diffusion models with notable success. Autonomous driv-
ing has adopted two main multimodal strategies: the first
uses discrete commands to guide trajectory generation, such
as in VAD[17], which produces three distinct trajectory
modes, and SparseDrive[27] and [16], which cluster fixed
navigation points from datasets for trajectory guidance. The
second approach introduces diffusion models directly to
generate multimodal trajectories[18, 29, 32], achieving suc-
cess in trajectory prediction but facing challenges in end-to-
end applications. Building on diffusion models, we address
limitations in accuracy and efficiency by incorporating flow
matching, using goal points to guide trajectories with preci-
sion rather than focusing solely on multimodal diversity.
3. Method
3.1. Preliminary
Compared to diffusion, which focuses on learning to reverse
the gradual addition of noise over time to recover data, flow
matching[23] focuses on learning invertible transformations
that map between data distributions. Let Ï€0 denote a sim-
ple distribution, typically the standard normal distribution
p(x) = N(x|0, I), and let Ï€1 denote the target distribu-
tion. Under this framework, rectified flow[24] uses a simple
and effective method to construct the path through optimal
transport[25] displacement, which we choose as our Flow
Matching method.
Given x0 sampled from Ï€0, x1 sampled from Ï€1, and
t âˆˆ[0, 1], the path from x0 to x1 is defined as a straight line,
meaning the intermediate status xt is given by (1 âˆ’t)x0 +
tx1, with the direction of intermediate status consistently
following x1 âˆ’x0. By constructing a neural network vÎ¸
to predict the direction x1 âˆ’x0 based on the current state
xt and time step t, we can obtain a path from the initial
distribution Ï€0 to target distribution Ï€1 by optimizing the
loss between vÎ¸(xt, t) and x1 âˆ’x0. This can be formalized
as:
vÎ¸(xt, t) â‰ˆEx0âˆ¼Ï€0,x1âˆ¼Ï€1[vt|xt]
(1)
L(Î¸) = Ex0âˆ¼Ï€0,x1âˆ¼Ï€1[âˆ¥vÎ¸(xt, t) âˆ’(x1 âˆ’x0)âˆ¥2]
(2)
3.2. GoalFlow
3.2.1. Overview
GoalFlow is a goal-driven end-to-end autonomous driving
method that can generate high-quality multimodal trajecto-
ries. The overall architecture of GoalFlow is illustrated in

=== Page 4 ===
Figure 2. It comprises three main components. In the Per-
ception Module, we obtain a BEV feature Fbev that encap-
sulates environmental information by fusing camera images
I, and LiDAR data L. The Goal Point Construction Mod-
ule focuses on generating precise guidance information for
trajectory generation. It accomplishes this by constructing a
goal point vocabulary V = {gi}N, and employing a scoring
mechanism to select the most appropriate goal point g. In
the Trajectory Planning Module, we produce a set of multi-
modal trajectories, T = {Ë†Ï„i}M, and then identify the opti-
mal trajectory Ï„, through a trajectory scoring mechanism.
3.2.2. Perception Module
In the first step, we fuse image and LiDAR data to create a
BEV feature, Fbev, that captures rich road condition infor-
mation. A single modality often lacks crucial details; for
example, LiDAR does not capture traffic light information,
while images cannot precisely locate objects. By fusing dif-
ferent sensor modalities, we can achieve a more complete
and accurate representation of the road conditions.
We adopt the Transfuser architecture [3] for modality fu-
sion. The forward, left, and right camera views are con-
catenated into a single image I âˆˆR3Ã—H1Ã—W1, while Li-
DAR data is formed as a tensor L âˆˆRKÃ—3. These in-
puts are passed through separate backbones, and their fea-
tures are fused at different layers using multiple transformer
blocks. The result is a BEV feature, Fbev, which compre-
hensively represents the scene. To ensure effective inter-
action between the ego vehicle and surrounding objects, as
well as map information, we apply auxiliary supervision to
the BEV feature through losses derived from HD maps and
bounding boxes.
3.2.3. Goal Point Construction Module.
In this module, we construct a precise goal point to
guide the trajectory generation process.
Diffusion-based
approach[18, 32] without constraints often leads to exces-
sive trajectory divergence, which complicates trajectory se-
lection. Our key observation is that a goal point contains a
precise description of the short-term future position, which
imposes a strong constraint on the generation model. As a
result, we divide the traditional Planning Module into two
steps: first, constructing a precise goal point, and second,
generating the trajectory through planning.
Goal Point Vocabulary.
We aim to construct a goal
point set that provides candidates for the optimal goal point.
Traditional goal-based methods[11, 34], rely on lane-level
information from HD map to generate goal point sets for
trajectory prediction. However, HD maps are expensive,
making lane information often unavailable in end-to-end
driving. Inspired by VADv2[1], we discretize the endpoint
space of trajectories to generate candidate goal points, en-
abling a solution without relying on HD maps. We clustered
trajectory endpoints pi = (xi, yi, Î¸i) in the training data
to create N cluster centers, which form our goal point vo-
cabulary V. Each endpoint pi represents a position (xi, yi)
and heading Î¸i. To ensure that the vocabulary represents
finer-grained locations, we typically set N to a large value,
generally 4096 or 8192.
Goal Point Scorer. High-quality trajectories typically
exhibit the following characteristics: A small distance to the
ground truth and within the drivable area. To achieve this,
we evaluate each goal point gi in the vocabulary V using
two distinct scores: the Distance Score Ë†Î´dis and the Drivable
Area Compliance Score Ë†Î´dac. The Distance Score measures
the proximity between the goal point gi and the endpoint of
ground truth trajectory ggt, with a continuous value in the
range Ë†Î´dis âˆˆ[0, 1], where a higher value indicates a closer
match to ggt. The Drivable Area Compliance Score ensures
that the goal point lies within the drivable area, using a bi-
nary value Ë†Î´dac âˆˆ{0, 1}, where 1 indicates that the goal
point is valid within the drivable area, and 0 indicates it is
not.
To construct the target distance score Î´dis
i , we utilize the
softmax function to map the Euclidean distance between the
goal point gi and the ground truth goal point ggt to the inter-
val [0, 1]. This is defined as:
Î´dis
i
=
exp(âˆ’âˆ¥gi âˆ’ggtâˆ¥2)
P
j exp(âˆ’âˆ¥gj âˆ’ggtâˆ¥2)
(3)
For the target drivable area compliance score Î´dac
i , we
introduce a shadow vehicle, whose bounding box is deter-
mined based on the position and heading (xi, yi, Î¸i) in gi
and the shape of the ego vehicle. Let {pj}4 represent the
set of four corner positions of the shadow vehicle, and let
D denote the polygon representing the drivable area. The
drivable area compliance score Î´dac
i
is defined as:
Î´dac
i
=
(
1,
if âˆ€j, pj âˆˆDâ—¦
0,
otherwise
We compute the final score Ë†Î´final
i
by aggregating Ë†Î´dis
i
and
Ë†Î´dac
i . The goal point with the highest final score is selected
for trajectory generation.
Ë†Î´final
i
= w1 log Ë†Î´dis
i
+ w2 log Ë†Î´dac
i
As shown in Fig.3(a), the Transformer-based Scorer De-
coder uses the result of adding Fv and Fego as the query,
with Fbev as the key and value. The output is passed through
two separate MLPs to produce the scores Ë†Î´dis and Ë†Î´dac for
each point in the V. Fig.3(b) shows the distribution of these
two scores. With the points in warmer colors representing
higher scores, we observe that score Ë†Î´dis effectively indi-
cates the desired future position, while Ë†Î´dac identifies if the
goal point is within the drivable area.

=== Page 5 ===
Vocab.
Encoder
Ego
Status
Ego
Encoder
Transformer Decoder
MLP
MLP
V
ğ¸
ğ¹ğ‘‰
ğ¹ğ‘’ğ‘”ğ‘œ
ğ¹ğ‘ğ‘’ğ‘£
{ Î´ğ‘–
ğ‘‘ğ‘–ğ‘ }ğ‘
{ Î´ğ‘–
ğ‘‘ğ‘ğ‘}ğ‘
(a)
{Î´ğ‘–
ğ‘‘ğ‘–ğ‘ }ğ‘
{Î´ğ‘–
ğ‘‘ğ‘ğ‘}ğ‘
ğ›¿ğ‘–
ğ‘‘ğ‘–ğ‘ =
ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(-dğ‘–)
ğ›¿ğ‘–
ğ‘‘ğ‘ğ‘=0
ğ›¿ğ‘—
ğ‘‘ğ‘ğ‘=1
{ Î´ğ‘–
ğ‘‘ğ‘–ğ‘ }ğ‘
{ Î´ğ‘–
ğ‘‘ğ‘ğ‘}ğ‘
(b)
Figure 3. Goal Point Scorer. (a) shows the detailed structure of the Goal Point Construction Module, and (b) presents the score distributions
of {Ë†Î´dis
i
}N, {Ë†Î´dac
i
}N, and {Ë†Î´final
i
}N, where points with higher scores are highlighted with warmer color.
Trajectory
Encoder
Environment 
Encoder
Time Step
Encoder
Transformer Layer
ğ‘„
ğ¹ğ‘”ğ‘œğ‘ğ‘™
ğ‘
 ğ‘£ğ‘¡
ğ¹ğ‘’ğ‘›ğ‘£
ğ¹ğ‘¡ğ‘Ÿğ‘ğ‘—
ğ¹ğ‘¡
ğ¹ğ‘ğ‘™ğ‘™
ğ¹ğ‘’ğ‘”ğ‘œ
Condition
Transformer Layer
Transformer Layer
Transformer Layer
ğ‘¡
ğ¹ğ‘ğ‘’ğ‘£
ğ‘‡ğ‘Ÿğ‘ğ‘—ğ‘¡
Positional
Encoding
Figure 4. The network architecture used in Rectified Flow.
3.2.4. Trajectory Planning Module
In this module, we generate constrained, high-quality tra-
jectory candidates using a generative model and then select
the optimal trajectory through a scoring mechanism. Gen-
erative models based on diffusion methods like DDPM[14]
and DDIM[26] typically require complex denoising paths,
leading to significant time overhead during inference, which
makes them unsuitable for real-time systems like au-
tonomous driving. In contrast, Rectified Flow[24], which
is based on the optimal transport path in flow matching, re-
quires much fewer inference steps to achieve good results.
We adopt Rectified Flow as the generative model, using the
BEV feature and goal point as conditions to generate multi-
modal trajectories.
Multimodal Trajectories Generating.
We generate
multimodal trajectories by modeling the shift from the noise
distribution to the target trajectory distribution. During this
distribution transfer process, given the current state xt and
time step t, we predict the shift vt.
vt = Ï„ norm âˆ’x0
(4)
xt = (1 âˆ’t)x0 + tÏ„ norm
(5)
Ï„ norm = H(Ï„ gt)
(6)
Where Ï„ gt is the ground truth trajectory and Ï„ norm is its
normalized form. We define H(Â·) as the normalization op-
eration applied to the trajectory. The variable x0 represents
the noise distribution, which follows x0 âˆ¼N(0, Ïƒ2I). The
variable xt is obtained by linearly interpolating between x0
and Ï„ norm.
As illustrated in Fig.4, we extract different features
through a series of encoders. Specifically, we encode xt us-
ing a linear layer, while t and the goal point are transformed
into feature vectors via sinusoidal encoding. The feature
Fenv is obtained by passing the information from Fbev and
Fego through the environment encoder.
Fenv = Eenv(Q, (FBEV + Fego), (FBEV + Fego))
(7)
Here, Eenv refers to a Transformer-based encoder, Q de-
notes a learnable embedding, and Fego represents the ego
status feature, which encodes the kinematic information of
the ego vehicle.
We concatenate the features Fenv, Fgoal, Ftraj, and Ft to
form the overall feature Fall, which encapsulates the current
state, time step, and scene information. This combined fea-
ture is then passed through several attention layers to predict
the distribution shift vt.
Ë†vt = G(Fall, Fall, Fall)
(8)

=== Page 6 ===
Fall = Concat(Fenv, Fgoal, Ftraj, Ft)
(9)
Where G is the network that consists of N attention layers.
We reconstruct the trajectory distribution using x0 and
Ë†vt. Typically, we achieve this by performing multiple in-
ference steps through the Rectified Flow, gradually trans-
forming the noise distribution x0 to the target distribution
Ï„ norm. Finally, we apply denormalization to Ï„ norm to obtain
the final trajectory Ë†Ï„.
Ë†Ï„ = Hâˆ’1(Ë†Ï„ norm)
(10)
Ë†Ï„ norm = x0 + 1
n
n
X
i
Ë†vti
(11)
Where n is the total inference steps, and ti is the time
step sampled in the i-th step, which satisfies ti âˆˆ[0, 1].
Hâˆ’1(Â·) is the denormalization operation.
Trajectory Selecting In trajectory selection, methods
like SparseDrive[27] and Diffusion-ES[32] rely on kine-
matic simulation of the generated trajectories to predict po-
tential collisions with surrounding agents, thus selecting the
optimal trajectory. This process significantly increases the
inference time. We simplify this procedure by using the
goal point as a reference for selecting the trajectory. Specif-
ically, we trade off the trajectory distance to the goal point
and ego progress, selecting the optimal trajectory through a
trajectory scorer.
f(Ë†Ï„i) = âˆ’Î»1Î¦(fdis(Ë†Ï„i)) + Î»2Î¦(fpg(Ë†Ï„i))
(12)
where Î¦(Â·) is the minimax operation. fdis(Ë†Ï„i) presents the
L2 distance of Ë†Ï„i and g, and fpg(Ë†Ï„i) presents the L2 distance
of progress of Ë†Ï„i make.
Furthermore, predicted goal point may contain an error
that can misguide the trajectory. To mitigate this, we mask
the goal point during generation to create a shadow trajec-
tory. If the shadow trajectory deviates significantly from the
main trajectory, we treat the goal point as unreliable and use
the shadow as the output.
3.2.5. Training Losses
Firstly, we optimize the perception extractor exclusively,
and enforce multiple perception losses for supervision, in-
cluding the cross-entropy loss for HD map (LHD) and 3D
bounding box classification (Lbbox) and L1 loss for 3D
bounding box locations (Lloc). This stage aims to enrich
the BEV feature with information on various perceptions.
Losses are as follows.
Lperception = w1 âˆ—LHD + w2 âˆ—Lbbox + w3 âˆ—Lloc (13)
where w1, w2, w3 are set to 10.0, 1.0, 10.0 in training. For
the goal constructor, we employ the cross entropy loss for
distance score(Ldis) and DAC score(Ldac). w4, w5 are set
to 1.0 and 0.005.
Lgoal = w4 âˆ—Ldis + w5 âˆ—Ldac
(14)
Ldis = âˆ’
N
X
i=1
Î´dis
i
log(Ë†Î´i
dis)
(15)
Ldac = âˆ’Î´daclogË†Î´dac âˆ’(1 âˆ’Î´dac)log(1 âˆ’Ë†Î´dac)
(16)
L1 loss is utilized for multimodal planner.
Lplanner = |vt âˆ’Ë†vt|
(17)
4. Experiments
4.1. Dataset
Our experiment is validated on the Openscene[6] dataset.
Openscene includes 120 hours of autonomous driving data.
Its end-to-end environment Navsim[7] uses 1192 and 136
scenarios for trainval and testing, a total of over 10w sam-
ples at 2Hz. Each sample contains camera images from 8
perspectives, fused Lidar data from 5 sensors, ego status,
and annotations for the map and objects.
4.2. Metrics
In the Navsim environment, the generated 2Hz, 4-second
trajectories are interpolated via an LQR controller to yield
10Hz, 4-second trajectories. These trajectories are scored
using closed-loop metrics, including No at-fault Collisions
SNC, Drivable Area Compliance SDAC, Time to Collision
ST T C with bounds, Ego Progress SEP , Comfort SCF , and
Driving Direction Compliance SDDC. The final score is
derived by aggregating these metrics. Due to practical con-
straints, SDDC is omitted from the calculation1.
SP DM = SNC Ã— SDAC Ã— sT T CÃ—
5 Ã— SEP + 5 Ã— SCF + 2 Ã— SDDC
12

(18)
4.3. Baselines
In Navsim, we compare against the following baselines:
Constant Velocity Assumes constant speed from the cur-
rent timestamp for forward movement. Ego Status MLP
Takes only the current state as input and uses an MLP
to generate the trajectory.
PDM-Closed Using ground-
truth perception as input, several trajectories are gener-
ated through a rule-based IDM method. The PDM scorer
then selects the optimal trajectory from these as the out-
put. Transfuser Uses both image and LiDAR inputs, fus-
ing them via a transformer into a BEV feature, which is then
used for trajectory generation. LTF A streamlined version
1https://github.com/autonomousvision/navsim/issues/14
